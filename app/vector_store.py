import chromadb
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Optional
import uuid
import os
import re
from collections import Counter

from .config import CHROMA_PERSIST_DIR, EMBEDDING_MODEL

class VectorStore:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB –∏ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è ChromaDB –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs(CHROMA_PERSIST_DIR, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ChromaDB
        try:
            self.chroma_client = chromadb.PersistentClient(
                path=str(CHROMA_PERSIST_DIR)
            )
            print("‚úÖ ChromaDB –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ChromaDB: {e}")
            raise
        
        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é
        try:
            self.collection = self.chroma_client.get_collection("history_textbooks")
            print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è, —á–∞–Ω–∫–æ–≤: {self.collection.count()}")
        except:
            self.collection = self.chroma_client.create_collection(
                name="history_textbooks",
                metadata={"hnsw:space": "cosine"}
            )
            print("‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {EMBEDDING_MODEL}")
        try:
            self.embedding_model = SentenceTransformer(EMBEDDING_MODEL)
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.embedding_model.get_sentence_embedding_dimension()}")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            print("üîÑ –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∞–Ω–≥–ª–∏–π—Å–∫—É—é –º–æ–¥–µ–ª—å...")
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å")
    
    def add_chunks(self, chunks: List[Dict[str, Any]], doc_id: int) -> List[str]:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫–∏ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ ID —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
        """
        if not chunks:
            return []
        
        embeddings = []
        metadatas = []
        ids = []
        documents = []
        
        print(f"üîÑ –î–æ–±–∞–≤–ª—è–µ–º {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ ChromaDB...")
        
        for i, chunk in enumerate(chunks):
            try:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
                chunk_id = f"doc{doc_id}_chunk{i}_{uuid.uuid4().hex[:8]}"
                
                # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                embedding = self.embedding_model.encode(chunk["content"]).tolist()
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å—Ç—Ä–æ–∫–∞–º–∏)
                metadata = {
                    "doc_id": str(doc_id),
                    "chunk_index": str(i),
                    "page_number": str(chunk.get("page_number", 1)),
                    "chapter": str(chunk.get("chapter", ""))[:100],
                    "paragraph": str(chunk.get("paragraph", ""))[:100],
                    "section_title": str(chunk.get("section_title", ""))[:200],
                    "id": str(i)  # –î–æ–±–∞–≤–ª—è–µ–º ID –¥–ª—è –ø–æ–∏—Å–∫–∞
                }
                
                embeddings.append(embedding)
                metadatas.append(metadata)
                ids.append(chunk_id)
                documents.append(chunk["content"][:1000])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è ChromaDB
                
                if i % 50 == 0 and i > 0:
                    print(f"  ‚è≥ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(chunks)} —á–∞–Ω–∫–æ–≤")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —á–∞–Ω–∫–∞ {i}: {e}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é –±–∞—Ç—á–∞–º–∏ –ø–æ 100
        batch_size = 100
        added_count = 0
        
        for i in range(0, len(embeddings), batch_size):
            try:
                batch_end = min(i + batch_size, len(embeddings))
                self.collection.add(
                    embeddings=embeddings[i:batch_end],
                    metadatas=metadatas[i:batch_end],
                    ids=ids[i:batch_end],
                    documents=documents[i:batch_end]
                )
                added_count += (batch_end - i)
                print(f"  ‚úì –î–æ–±–∞–≤–ª–µ–Ω –±–∞—Ç—á {i//batch_size + 1}/{(len(embeddings)-1)//batch_size + 1}")
            except Exception as e:
                print(f"  ‚úó –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –±–∞—Ç—á–∞: {e}")
                # –ü—Ä–æ–±—É–µ–º –¥–æ–±–∞–≤–∏—Ç—å –ø–æ –æ–¥–Ω–æ–º—É
                for j in range(i, min(i + batch_size, len(embeddings))):
                    try:
                        self.collection.add(
                            embeddings=[embeddings[j]],
                            metadatas=[metadatas[j]],
                            ids=[ids[j]],
                            documents=[documents[j]]
                        )
                        added_count += 1
                    except Exception as e2:
                        print(f"    ‚úó –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —á–∞–Ω–∫–∞ {j}: {e2}")
        
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ {added_count}/{len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ ChromaDB")
        return ids[:added_count]
    
    def get_collection_stats(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        try:
            count = self.collection.count()
            return {
                "total_chunks": count,
                "collection_name": self.collection.name,
                "status": "active"
            }
        except Exception as e:
            return {
                "total_chunks": 0,
                "collection_name": "history_textbooks",
                "status": f"error: {e}"
            }
    
    def search(self, query: str, n_results: int = 5) -> Optional[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —á–∞–Ω–∫–æ–≤"""
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.embedding_model.encode(query).tolist()
            
            # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ —á–∞–Ω–∫–∏
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                include=["metadatas", "documents", "distances"]
            )
            
            return results
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def delete_document(self, doc_id: int):
        """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            self.collection.delete(
                where={"doc_id": str(doc_id)}
            )
            print(f"‚úÖ –£–¥–∞–ª–µ–Ω—ã —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id} –∏–∑ ChromaDB")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}: {e}")

    def _load_embedding_model(self):
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if self.embedding_model is None:
            print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {EMBEDDING_MODEL}")
            try:
                self.embedding_model = SentenceTransformer(EMBEDDING_MODEL)
                print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.embedding_model.get_sentence_embedding_dimension()}")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                print("üîÑ –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∞–Ω–≥–ª–∏–π—Å–∫—É—é –º–æ–¥–µ–ª—å...")
                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∞—è –º–æ–¥–µ–ª—å")
        return self.embedding_model
    
    def _extract_keywords(self, query: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        """
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        query_lower = query.lower()
        
        # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        query_lower = re.sub(r'[^\w\s]', ' ', query_lower)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words = query_lower.split()
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (–∫–æ—Ä–æ—Ç–∫–∏–µ –∏ —á–∞—Å—Ç–æ—Ç–Ω—ã–µ)
        stop_words = {'–∫–æ–≥–¥–∞', '–≥–¥–µ', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–æ–µ', '–∫–∞–∫–∏–µ', '—á—Ç–æ', '–∫—Ç–æ', 
                     '–∫–∞–∫', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '—Å–∫–æ–ª—å–∫–æ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–æ', '—ç—Ç–∏',
                     '–≤–µ—Å—å', '–≤—Å—è', '–≤—Å–µ', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏', '–ø—Ä–∏', '–¥–ª—è',
                     '—á—Ç–æ–±—ã', '—á—Ä–µ–∑', '—á–µ—Ä–µ–∑', '–æ–∫–æ–ª–æ', '–ø–æ—á—Ç–∏', '—É–∂–µ', '–µ—â–µ', '–µ—â—ë'}
        
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ 3 —Å–∏–º–≤–æ–ª–æ–≤ –∏ –Ω–µ –≤ —Å—Ç–æ–ø-–ª–∏—Å—Ç–µ
        keywords = [word for word in words if len(word) > 3 and word not in stop_words]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ –¥–ª—è –∏–º–µ–Ω (–¶–µ–∑–∞—Ä—å -> —Ü–µ–∑–∞—Ä, —é–ª–∏–π)
        variations = []
        for word in keywords:
            if word in ['—Ü–µ–∑–∞—Ä—å', '—Ü–µ–∑–∞—Ä—è', '—Ü–µ–∑–∞—Ä—é', '—Ü–µ–∑–∞—Ä–µ–º']:
                variations.extend(['—Ü–µ–∑–∞—Ä', '—é–ª–∏–π'])
            if word in ['—é–ª–∏–π', '—é–ª–∏—è']:
                variations.append('—é–ª–∏–π')
                
        keywords.extend(variations)
        
        return list(set(keywords))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    def _keyword_search_sql(self, keywords: List[str], n_results: int) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —á–µ—Ä–µ–∑ SQL —Å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        """
        from .database import get_db, Chunk
        from sqlalchemy import or_, and_
        
        db = get_db()
        try:
            if not keywords:
                return []
            
            # –°–æ–∑–¥–∞–µ–º —É—Å–ª–æ–≤–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
            conditions = []
            for word in keywords:
                # –ò—â–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞
                conditions.append(Chunk.content.ilike(f'%{word}%'))
                conditions.append(Chunk.content.ilike(f'%{word.capitalize()}%'))
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            chunks = db.query(Chunk).filter(
                or_(*conditions)
            ).limit(n_results * 2).all()  # –ë–µ—Ä–µ–º —Å –∑–∞–ø–∞—Å–æ–º
            
            # –†–∞–Ω–∂–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–π
            ranked_chunks = []
            for chunk in chunks:
                content_lower = chunk.content.lower()
                score = 0
                
                # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–∞–π–¥–µ–Ω–æ
                found_keywords = []
                for word in keywords:
                    if word in content_lower:
                        score += 1
                        found_keywords.append(word)
                        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–µ—Å –∑–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                        if f" {word} " in f" {content_lower} ":
                            score += 1
                
                # –û—Å–æ–±—ã–π –≤–µ—Å –¥–ª—è –∏–º–µ–Ω
                if '—é–ª–∏–π' in found_keywords or '—Ü–µ–∑–∞—Ä' in found_keywords:
                    score += 3
                
                if score > 0:
                    ranked_chunks.append({
                        'content': chunk.content,
                        'metadata': {
                            'doc_id': str(chunk.doc_id),
                            'page_number': str(chunk.page_number),
                            'chapter': chunk.chapter or '',
                            'paragraph': chunk.paragraph or '',
                            'id': chunk.id
                        },
                        'score': score,
                        'source': 'keyword',
                        'keywords_found': found_keywords
                    })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å–∫–æ—Ä–∞
            ranked_chunks.sort(key=lambda x: x['score'], reverse=True)
            
            return ranked_chunks[:n_results]
            
        finally:
            db.close()
    
    def hybrid_search(self, query: str, n_results: int = 5, vector_weight: float = 0.4):
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
        """
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = self._extract_keywords(query)
        print(f"üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords}")
        
        # 2. –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (SQL)
        keyword_results = self._keyword_search_sql(keywords, n_results)
        
        # 3. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        vector_results = self.search(query, n_results=n_results * 2)
        
        # 4. –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        combined_chunks = []
        seen_ids = set()
        
        # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –∫–ª—é—á–µ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –∏–º–µ–Ω)
        for chunk in keyword_results:
            chunk_id = chunk['metadata'].get('id')
            if chunk_id not in seen_ids:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º score –≤ –¥–∏–∞–ø–∞–∑–æ–Ω 0-1
                max_keyword_score = max([c['score'] for c in keyword_results]) if keyword_results else 1
                norm_score = chunk['score'] / max_keyword_score
                
                combined_chunks.append({
                    'content': chunk['content'],
                    'metadata': chunk['metadata'],
                    'score': norm_score,
                    'source': 'keyword',
                    'keywords': chunk.get('keywords_found', [])
                })
                seen_ids.add(chunk_id)
        
        # –ó–∞—Ç–µ–º –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        if vector_results and vector_results.get('documents'):
            for i, doc in enumerate(vector_results['documents'][0]):
                meta = vector_results['metadatas'][0][i]
                chunk_id = meta.get('id', i)
                
                if chunk_id not in seen_ids:
                    distance = vector_results['distances'][0][i] if vector_results.get('distances') else 0
                    vector_score = 1.0 - min(distance, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
                    
                    combined_chunks.append({
                        'content': doc,
                        'metadata': meta,
                        'score': vector_score,
                        'source': 'vector'
                    })
                    seen_ids.add(chunk_id)
        
        # 5. –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Å –≤–µ—Å–∞–º–∏
        for chunk in combined_chunks:
            if chunk['source'] == 'keyword':
                # –î–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –æ—Å—Ç–∞–≤–ª—è–µ–º –≤—ã—Å–æ–∫–∏–π –≤–µ—Å
                chunk['final_score'] = chunk['score']
            else:
                # –î–ª—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö - —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º
                chunk['final_score'] = chunk['score'] * vector_weight
        
        combined_chunks.sort(key=lambda x: x['final_score'], reverse=True)
        
        # 6. –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        return combined_chunks[:n_results]