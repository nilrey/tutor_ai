import requests
import time

print("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama...")

# 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–µ—Ä–≤–µ—Ä
try:
    r = requests.get("http://localhost:11434/api/tags", timeout=5)
    print(f"‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω")
    models = r.json().get('models', [])
    for m in models:
        print(f"   - {m['name']}")
except Exception as e:
    print(f"‚ùå Ollama –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç: {e}")
    print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama –∏–∑ –º–µ–Ω—é –ü—É—Å–∫ –∏–ª–∏ –∫–æ–º–∞–Ω–¥–æ–π 'ollama serve'")
    exit()

# 2. –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
model_name = "gemma3:4b"  # –∏–ª–∏ llama3:latest
print(f"\nüîÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å {model_name}...")

try:
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": model_name,
            "prompt": "–û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º: 2+2=?",
            "stream": False,
            "options": {
                "num_predict": 10,
                "temperature": 0
            }
        },
        timeout=30
    )
    
    if response.status_code == 200:
        answer = response.json()['response']
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ—Ç–≤–µ—á–∞–µ—Ç: {answer}")
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞: {response.status_code}")
        
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {e}")