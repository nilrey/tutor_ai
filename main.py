from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
import shutil
from pathlib import Path
import uuid
import os
import warnings
warnings.filterwarnings("ignore")

from app.config import UPLOAD_DIR
from app.database import init_db, Document, Chunk, QALog
from app.document_processor import DocumentProcessor
from app.vector_store import VectorStore

from app.schemas import QuestionRequest, QuestionResponse, GenerateQuestionsRequest, GenerateQuestionsResponse
from app.agent import HistoryRAGAgent
from app.llm_client import LLMClient
import time


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
app = FastAPI(title="History AI Tutor - Document Processor")

# –ü–û–†–Ø–î–û–ö –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–ò –í–ê–ñ–ï–ù!
# 1. –°–Ω–∞—á–∞–ª–∞ –ë–î
get_db = init_db()

# 2. –ü–æ—Ç–æ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
doc_processor = DocumentProcessor()

# 3. –ü–æ—Ç–æ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
vector_store = VectorStore()

# 4. –ü–û–¢–û–ú –∫–ª–∏–µ–Ω—Ç LLM (–ë–ï–ó api_key!)
print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama –∫–ª–∏–µ–Ω—Ç–∞...")
llm_client = LLMClient(
    model_name="gemma3:4b",  # –∏–ª–∏ "mistral", "gemma:7b"
    base_url="http://localhost:11434"
)

# 5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
if llm_client.is_available():
    models = llm_client.get_available_models()
    print(f"‚úÖ Ollama –¥–æ—Å—Ç—É–ø–Ω–∞. –ú–æ–¥–µ–ª–∏: {models}")
else:
    print("‚ö†Ô∏è Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞! –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Ä–µ–∂–∏–º –∑–∞–≥–ª—É—à–∫–∏ (mock)")

# 6. –ò —Ç–æ–ª—å–∫–æ –ø–æ—Ç–æ–º –∞–≥–µ–Ω—Ç
rag_agent = HistoryRAGAgent(
    vector_store=vector_store, 
    llm_client=llm_client
)

# –ü–æ–ª—É—á–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ—Å—Å–∏–π –ë–î
get_db = init_db()  # init_db() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é get_db
doc_processor = DocumentProcessor()
vector_store = VectorStore()


# --- –≠–ù–î–ü–û–ò–ù–¢–´ –î–õ–Ø AI –ê–ì–ï–ù–¢–ê ---
@app.post("/ask", response_model=QuestionResponse)
async def ask_question(request: QuestionRequest):
    """
    –ó–∞–¥–∞—Ç—å —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ —É—á–µ–±–Ω–∏–∫—É.
    """
    try:
        result = rag_agent.answer_fact(
            query=request.query,
            document_id=request.document_id,
            top_k=request.top_k
        )
        return QuestionResponse(**result)
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞: {str(e)}")

@app.post("/generate-questions", response_model=GenerateQuestionsResponse)
async def generate_questions(request: GenerateQuestionsRequest):
    """
    –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –ø–∞—Ä–∞–≥—Ä–∞—Ñ—É.
    """
    try:
        result = rag_agent.generate_questions(
            document_id=request.document_id,
            chapter=request.chapter,
            paragraph=request.paragraph,
            num_questions=request.num_questions
        )
        
        if "error" in result:
            raise HTTPException(404, result["error"])
            
        return GenerateQuestionsResponse(**result)
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(500, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤: {str(e)}")

@app.get("/documents")
async def list_documents():
    """
    –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —É—á–µ–±–Ω–∏–∫–æ–≤.
    """
    db = get_db()
    try:
        docs = db.query(Document).all()
        return {
            "documents": [
                {
                    "id": d.id,
                    "filename": d.filename,
                    "upload_date": d.upload_date.isoformat() if d.upload_date else None,
                    "chunks": d.total_chunks,
                    "chapters": []  # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                }
                for d in docs
            ]
        }
    finally:
        db.close()

@app.get("/documents/{doc_id}/structure")
async def get_document_structure(doc_id: int):
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É —É—á–µ–±–Ω–∏–∫–∞ (–≥–ª–∞–≤—ã, –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã).
    """
    db = get_db()
    try:
        chunks = db.query(Chunk).filter(Chunk.doc_id == doc_id).all()
        
        chapters = set()
        paragraphs = set()
        
        for chunk in chunks:
            if chunk.chapter:
                chapters.add(chunk.chapter)
            if chunk.paragraph:
                paragraphs.add(chunk.paragraph)
        
        return {
            "document_id": doc_id,
            "chapters": sorted(list(chapters)),
            "paragraphs": sorted(list(paragraphs)),
            "total_chunks": len(chunks)
        }
    finally:
        db.close()


@app.post("/upload")
async def upload_document(file: UploadFile = File(...)):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç PDF —É—á–µ–±–Ω–∏–∫, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –µ–≥–æ."""
    if not file.filename.endswith('.pdf'):
        raise HTTPException(400, "–¢–æ–ª—å–∫–æ PDF —Ñ–∞–π–ª—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è")
    
    temp_file_path = None
    
    try:
        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        file_extension = Path(file.filename).suffix
        safe_filename = f"{uuid.uuid4()}{file_extension}"
        file_path = UPLOAD_DIR / safe_filename
        temp_file_path = file_path
        
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        print(f"üíæ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {file_path}")
        
        # 2. –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –≤ –ë–î - –í–´–ó–´–í–ê–ï–ú get_db() –ö–ê–ö –§–£–ù–ö–¶–ò–Æ
        db = get_db()
        try:
            document = Document(
                filename=file.filename,
                file_path=str(file_path)
            )
            db.add(document)
            db.commit()
            db.refresh(document)
            
            # 3. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            processed_data = doc_processor.process_document(
                file_path=str(file_path),
                filename=file.filename
            )
            
            # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ –≤ SQL
            for chunk_data in processed_data["chunks"]:
                chunk = Chunk(
                    doc_id=document.id,
                    content=chunk_data["content"],
                    page_number=chunk_data.get("page_number", 1),
                    chapter=chunk_data.get("chapter", ""),
                    paragraph=chunk_data.get("paragraph", ""),
                    section_title=chunk_data.get("section_title", ""),
                    chunk_index=chunk_data["chunk_index"]
                )
                db.add(chunk)
            db.commit()
            
            # 5. –î–æ–±–∞–≤–ª—è–µ–º –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
            embedding_ids = vector_store.add_chunks(
                processed_data["chunks"],
                document.id
            )
            
            # 6. –û–±–Ω–æ–≤–ª—è–µ–º —á–∞–Ω–∫–∏ —Å ID —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            if embedding_ids:
                for chunk_data, emb_id in zip(processed_data["chunks"], embedding_ids):
                    db.query(Chunk).filter(
                        Chunk.doc_id == document.id,
                        Chunk.chunk_index == chunk_data["chunk_index"]
                    ).update({"embedding_id": emb_id})
                db.commit()
            
            # 7. –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document.total_chunks = len(processed_data["chunks"])
            db.commit()
            
            return JSONResponse({
                "status": "success",
                "document_id": document.id,
                "filename": file.filename,
                "total_pages": processed_data["total_pages"],
                "total_chunks": len(processed_data["chunks"]),
                "chapters_found": len(processed_data.get("chapters", [])),
                "paragraphs_found": len(processed_data.get("paragraphs", [])),
                "message": "–£—á–µ–±–Ω–∏–∫ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω"
            })
            
        finally:
            db.close()
            
    except Exception as e:
        import traceback
        traceback.print_exc()
        # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ
        if temp_file_path and temp_file_path.exists():
            temp_file_path.unlink()
        raise HTTPException(500, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {str(e)}")


@app.get("/stats")
async def get_stats():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
    db = get_db()
    
    try:
        docs = db.query(Document).all()
        total_chunks = db.query(Chunk).count()
        
        vector_stats = vector_store.get_collection_stats()
        
        return {
            "documents": [
                {
                    "id": d.id,
                    "filename": d.filename,
                    "upload_date": d.upload_date.isoformat() if d.upload_date else None,
                    "chunks": d.total_chunks
                }
                for d in docs
            ],
            "total_documents": len(docs),
            "total_chunks_sql": total_chunks,
            "vector_db": vector_stats
        }
    finally:
        db.close()  # –í–∞–∂–Ω–æ –∑–∞–∫—Ä—ã–≤–∞—Ç—å —Å–µ—Å—Å–∏—é!

@app.get("/documents/{doc_id}/chunks")
async def get_document_chunks(doc_id: int, skip: int = 0, limit: int = 10):
    """–ü–æ–ª—É—á–∞–µ—Ç —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞"""
    db = get_db()  # –ë–´–õ–û: db = db_session()
    
    try:
        chunks = db.query(Chunk).filter(
            Chunk.doc_id == doc_id
        ).offset(skip).limit(limit).all()
        
        total = db.query(Chunk).filter(Chunk.doc_id == doc_id).count()
        
        return {
            "total": total,
            "skip": skip,
            "limit": limit,
            "chunks": [
                {
                    "id": c.id,
                    "content_preview": c.content[:200] + "..." if len(c.content) > 200 else c.content,
                    "page": c.page_number,
                    "chapter": c.chapter,
                    "paragraph": c.paragraph,
                    "title": c.section_title
                }
                for c in chunks
            ]
        }
    finally:
        db.close()

@app.on_event("startup")
async def startup_event():
    """–î–µ–π—Å—Ç–≤–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    print("üöÄ –ó–∞–ø—É—Å–∫ History AI Tutor")
    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∑–∞–≥—Ä—É–∑–æ–∫: {UPLOAD_DIR}")
    print(f"üóÑÔ∏è –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î: {vector_store.get_collection_stats()}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)