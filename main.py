from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
import shutil
from pathlib import Path
import uuid
import os
import warnings
warnings.filterwarnings("ignore")

from app.config import UPLOAD_DIR
from app.database import init_db, Document, Chunk
from app.document_processor import DocumentProcessor
from app.vector_store import VectorStore

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
app = FastAPI(title="History AI Tutor - Document Processor")

# –ü–æ–ª—É—á–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ—Å—Å–∏–π –ë–î
get_db = init_db()  # init_db() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é get_db
doc_processor = DocumentProcessor()
vector_store = VectorStore()

@app.post("/upload")
async def upload_document(file: UploadFile = File(...)):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç PDF —É—á–µ–±–Ω–∏–∫, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –µ–≥–æ."""
    if not file.filename.endswith('.pdf'):
        raise HTTPException(400, "–¢–æ–ª—å–∫–æ PDF —Ñ–∞–π–ª—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è")
    
    temp_file_path = None
    
    try:
        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        file_extension = Path(file.filename).suffix
        safe_filename = f"{uuid.uuid4()}{file_extension}"
        file_path = UPLOAD_DIR / safe_filename
        temp_file_path = file_path
        
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        print(f"üíæ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {file_path}")
        
        # 2. –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –≤ –ë–î - –í–´–ó–´–í–ê–ï–ú get_db() –ö–ê–ö –§–£–ù–ö–¶–ò–Æ
        db = get_db()
        try:
            document = Document(
                filename=file.filename,
                file_path=str(file_path)
            )
            db.add(document)
            db.commit()
            db.refresh(document)
            
            # 3. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            processed_data = doc_processor.process_document(
                file_path=str(file_path),
                filename=file.filename
            )
            
            # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ –≤ SQL
            for chunk_data in processed_data["chunks"]:
                chunk = Chunk(
                    doc_id=document.id,
                    content=chunk_data["content"],
                    page_number=chunk_data.get("page_number", 1),
                    chapter=chunk_data.get("chapter", ""),
                    paragraph=chunk_data.get("paragraph", ""),
                    section_title=chunk_data.get("section_title", ""),
                    chunk_index=chunk_data["chunk_index"]
                )
                db.add(chunk)
            db.commit()
            
            # 5. –î–æ–±–∞–≤–ª—è–µ–º –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
            embedding_ids = vector_store.add_chunks(
                processed_data["chunks"],
                document.id
            )
            
            # 6. –û–±–Ω–æ–≤–ª—è–µ–º —á–∞–Ω–∫–∏ —Å ID —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            if embedding_ids:
                for chunk_data, emb_id in zip(processed_data["chunks"], embedding_ids):
                    db.query(Chunk).filter(
                        Chunk.doc_id == document.id,
                        Chunk.chunk_index == chunk_data["chunk_index"]
                    ).update({"embedding_id": emb_id})
                db.commit()
            
            # 7. –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document.total_chunks = len(processed_data["chunks"])
            db.commit()
            
            return JSONResponse({
                "status": "success",
                "document_id": document.id,
                "filename": file.filename,
                "total_pages": processed_data["total_pages"],
                "total_chunks": len(processed_data["chunks"]),
                "chapters_found": len(processed_data.get("chapters", [])),
                "paragraphs_found": len(processed_data.get("paragraphs", [])),
                "message": "–£—á–µ–±–Ω–∏–∫ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω"
            })
            
        finally:
            db.close()
            
    except Exception as e:
        import traceback
        traceback.print_exc()
        # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ
        if temp_file_path and temp_file_path.exists():
            temp_file_path.unlink()
        raise HTTPException(500, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {str(e)}")


@app.get("/stats")
async def get_stats():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"""
    db = get_db()  # –ë–´–õ–û: db = db_session()
    
    try:
        docs = db.query(Document).all()
        total_chunks = db.query(Chunk).count()
        
        vector_stats = vector_store.get_collection_stats()
        
        return {
            "documents": [
                {
                    "id": d.id,
                    "filename": d.filename,
                    "upload_date": d.upload_date.isoformat() if d.upload_date else None,
                    "chunks": d.total_chunks
                }
                for d in docs
            ],
            "total_documents": len(docs),
            "total_chunks_sql": total_chunks,
            "vector_db": vector_stats
        }
    finally:
        db.close()  # –í–∞–∂–Ω–æ –∑–∞–∫—Ä—ã–≤–∞—Ç—å —Å–µ—Å—Å–∏—é!

@app.get("/documents/{doc_id}/chunks")
async def get_document_chunks(doc_id: int, skip: int = 0, limit: int = 10):
    """–ü–æ–ª—É—á–∞–µ—Ç —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞"""
    db = get_db()  # –ë–´–õ–û: db = db_session()
    
    try:
        chunks = db.query(Chunk).filter(
            Chunk.doc_id == doc_id
        ).offset(skip).limit(limit).all()
        
        total = db.query(Chunk).filter(Chunk.doc_id == doc_id).count()
        
        return {
            "total": total,
            "skip": skip,
            "limit": limit,
            "chunks": [
                {
                    "id": c.id,
                    "content_preview": c.content[:200] + "..." if len(c.content) > 200 else c.content,
                    "page": c.page_number,
                    "chapter": c.chapter,
                    "paragraph": c.paragraph,
                    "title": c.section_title
                }
                for c in chunks
            ]
        }
    finally:
        db.close()

@app.on_event("startup")
async def startup_event():
    """–î–µ–π—Å—Ç–≤–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    print("üöÄ –ó–∞–ø—É—Å–∫ History AI Tutor")
    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∑–∞–≥—Ä—É–∑–æ–∫: {UPLOAD_DIR}")
    print(f"üóÑÔ∏è –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î: {vector_store.get_collection_stats()}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)